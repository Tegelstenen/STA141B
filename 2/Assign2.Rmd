---
title: "STA141B—Assignment 2"
author: "Filip Wilhelm Sjostrand"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Create a data.frame 

To begin, we need to separate the five log files. We can do this by searching for the pattern suggested in the prompt: **# log-file-name**. By using the `grepl()` and `table()` functions, we iterated over various regex patterns until we found a match with five TRUE values. This led us to assume that log file names begin with '#'. The extracted file names are shown below. An error message "incomplete final line..." appears, but can be ignored in this case (Pon, 2011).

```{r}
# Packages ----------
library(readr)
library(dplyr)
source("/Users/filipsjostrand/Documents/UC Davis/Courses/STA 141B/assignments/2/functions.R")

# Get Data ----------
dir <- "/Users/filipsjostrand/Documents/UC Davis/Courses/STA 141B/data/asssign2"
log_file <- file.path(dir, "MergedAuth.log")
file.exists(log_file)
ll <- suppressWarnings(readLines(log_file))
head(ll)

# Identify Pattern ----------
rx <- "^#"
names <- grepl(rx, ll)
table(names)
ll[names]
```

After testing several approaches, we decided to split the tables by their file names, work on them separately, and then merge the final results. The `split_tables()` function confirmed that the tables were indeed split into five parts.

The `length(ll)` function returns `r length(ll)`, and after removing the five file names and the empty row at the top, we expected the total length of each table to equal 99962, which was accurate. However, this also included the empty row appearing before each file name. Thus, after removing those rows, the correct length should be 99958, which it is.

```{r}
# Remove file names & create data.frames
start_index <- grep(rx, ll)
tt <- split_tables(ll, start_index)
sum(lengths(tt)) 
tail(tt[[1]])
```

Now, let's start parsing the data, beginning with the date-time. Assuming that the date is formatted consistently across all files, we iterated over different regular expressions to find a pattern with no FALSE matches. The `rx_tester()` function returns nothing if only TRUE matches are found. All the dates match the identified pattern, which we then convert to POSIXct format by adding a dummy year. No errors were encountered during this process, confirming successful formatting.

```{r}
# Parsing date-time ----------
rx <- "[A-Z][a-z]{2} +[0-9]{1,2} [0-9]{2}:[0-9]{2}:[0-9]{2}"
res <- parser(tt, rx)
dummy <- lapply(res[[1]], paste, "2023")
dt <- lapply(dummy, as.POSIXct, format = "%b %d %H:%M:%S %Y")
```

Next, we want to parse the logging host. We find it easier to remove the previously parsed sections and perform a quick visual check to confirm successful removal of the date-time from the raw file. Assuming that there is a space between the date-time, logging host, and app, and no spaces within the logging host, we use `rx2` to find that `rx_tester()` returns nothing, indicating successful parsing. The sample does not suggest otherwise.

```{r}
# Parsing logging host ----------
rx2 <- "^ ([a-zA-Z0-9]|-)+ "
res2 <- parser(res[[2]], rx2)
log_host <- lapply(res2[[1]], trimws)
lapply(log_host, sample, 10)
```

For parsing the app and PID, we use a two-step process. First, we extract them as one chunk, then split within, also indicating missing PIDs. We start by removing the previously parsed sections. Visually, it seems that the app + PID chunk always ends with a ":". However, we have two cases to handle: some PIDs are missing, and some PIDs are followed by a chunk that belongs to the message part. By simply adding that we should read until the first occurrence of either ":" or "]", we resolve this issue.

```{r}
# Extracting PID and app ----------
rx3 <- "^.*?(\\]|:)"
res3 <- parser(res2[[2]], rx3)
messages <- lapply(res3[[2]], trimws)
lapply(res3[[1]], sample, 5)
```

Now, any entry that does not end with "]" needs to be appended with "[NA]". A first approach is to simply substitute ":" with "[NA]"—which works well. Then we split on "[...]"

```{r}
# Adding NAs
pid_app <- lapply(res3[[1]], gsub, pat = ":", repl = "[NA]")
pid_app <- lapply(pid_app, strsplit, split = "\\[|\\]", perl = T)
lapply(pid_app, sample, 2)
pid_app <- lapply(pid_app, function(x) do.call(rbind, x))
```

The rest of the process is straightforward. The last part of each line is the message. We create a data frame for each parsed section, add a column indicating which file the observation belongs to, and then merge them all. We can confirm that the length of the merged data frame has preserved the 99958 rows as discussed earlier.

```{r}
file_name <- gsub("# |\\.log", "", ll[names])

df_list <- list()
for (i in 1:5) {
    df_list[[i]] <- data.frame(
        `date-time` = dt[[i]],
        `logging host` = log_host[[i]],
        app = pid_app[[i]][,1],
        PID = pid_app[[i]][,2], message = messages[[i]]
    ) %>% mutate(file = file_name[i])
}
df <- do.call(rbind, df_list)
nrow(df)
```
# Data Validation and Exploration

## Verify that the PIDs are all numbers.

We can confirm that any observation in the PID column that is not a number is labeled as NA. Thus, we can safely transform it into a numerical column.

```{r}
logic <- grepl("[0-9]+", df$PID)
table(df$PID[!logic])
df$PID <- suppressWarnings(as.numeric(df$PID))
```
## How many lines are in each log file?

The table below displays the number of observations in each file.

```{r}
table(df$file)
```

## What are the range of date-times for the messages?

Ignoring the dummy year, we can confirm that the total span of the messages is from the 27th of March to the 31st of December. However, we cannot determine if this spans over different years since that information was lacking.

```{r}
summary(df$date.time)
```

### for each of the different log files in the combined file?

- **auth:** spans from 30th of November to 31st of December 

- **auth2:** spans from 27th of March to 20th of April.

- **loghub/Linux/Linux_2k:** spans from 14th of June to 27th of July.

- **loghub/Mac/Mac_2k:** spans from 1st of July to 8th of July.

- **loghub/OpenSSH/SSH_2k:** spans over the same day.

```{r}
df <- df %>%
    mutate_at(vars(file), as.factor)

tapply(df$date.time, df$file, summary)
```

### How many days does each log file span?

- **auth:** Time difference of 31.65889 days

- **auth2:** Time difference of 24.04691 days

- **loghub/Linux/Linux_2k:** Time difference of 42.97638 days

- **loghub/Mac/Mac_2k:** Time difference of 6.965174 days

- **loghub/OpenSSH/SSH_2k:** Time difference of 4.149722 hours

```{r}
s <- tapply(df$date.time, df$file, summary)
diffs <- Map(function(x) x[[6]] - x[[1]], s)
```

## Do the application names contain numbers? If so, are they just versions, e.g. ssh2, or is there additional structure to the numbers?

There are 11 occurrences of numbers within application names. According to Sumo Logic (n.d.), an application with the name containing "syslog 1.4.1" utilizes the syslog protocol for logging and operational tasks like audits, monitoring, and troubleshooting. They claim this application follows the standardized syslog architecture, which consists of content, application, and transport layers. On the other hand, Sesek (2016) states that "BezelServices 255.10" is likely a specific version of the Mac OS X subsystem connecting HID device drivers, preferences, and a UI surface for device feedback. He claims it supports devices like MagicTrackpad, Bluetooth keyboards, and IR remotes, dynamically selecting them through bundles in a specific directory.

```{r}
table(grepl("[0-9]", df$app))
df$app[grepl("[0-9]", df$app)]
```

## Is the host value constant/the same for all records in each log file?


It appears that the host value is constant in all files except for **loghub/Mac/Mac_2k**.

```{r}
tapply(df$logging.host, df$file, table)
```

## What are the most common apps (daemons/programs) that are logging information on each of the different hosts?

The table below displays the most common apps for each logging host, sorted by the highest occurrence to the lowest, displaying the top 10.

```{r}
df %>%
    group_by(logging.host, app) %>%
    count() %>%
    arrange(logging.host, desc(n)) %>%
    ungroup() %>%
    group_by(logging.host) %>%
    slice_max(n, n = 1) %>%
    arrange(desc(n)) %>% 
    head(10)
```
# Logins - valid and invalid

To see a pattern in how a valid message look like, we begin by visuallt observing the different messages. Then we create two columns containing what we think would be classified ass valid, invalid, or overload (too many attempts). Making sure we have no common elements we, check using `intersect()`. We also try to see what type of observations are left by removing any matches to either valid or invalid. To make it easier to identify which type of messages are left we shall also create a list of closing connections since it is acommon phenomena. By iterating through this process, and observing different entries in `truly_left`, we are able to identify three list of possible matches (with no intersections).

```{r}
valids <- matches(df$message,"(new group)|(Accepted)|(opened)|((C|c)onnection from [0-9])|(New session)|(new user)|(Server listening)|(Starting session)")

invalids <- matches(df$message,"(^Invalid)|(Did not)|(Could not)|(POSSIBLE)|(Failed password)|(input_userauth_request)|(Bad protocol version)|(Unable to negotiate)|(: authentication failure)|(Failed publickey)|^(authentication failure)|(user unknown)|(doesn't have account access)|(Failed none for)|(PAM [0-9] more)")

overload <- matches(df$message,"(many)|(maximum authentication)|(ignoring max retries)")

tot_match <- c(valids, invalids, overload)
left <- setdiff(df$message, tot_match)

closed <- matches(left,"(Bye Bye)|(session closed)|(Connection closed)|(Received disconnect)|(Disconnected from)|(reset by peer)|(Removed session)|(Close session)")

truly_left <- setdiff(left, closed)
length(left)
intersect(valids, invalids)
intersect(overload, invalids)
intersect(overload, valids)

```

## Find valid/successful logins, what are the user names and Internet Protocol (IP) addresses of those logins?

In the `tester` variable we try different patterns, check how many unique occurrences there are and from there on try to extract user or IP. For instance, a common pattern is "session opened for user" proceeded by a username. 

```{r}
head(valids)
tester <- matches(valids, "session opened for user")
names <- str_extract(tester, "(?<=session opened for user )([^ ]+)")
users <- unique(names)
users
```

Similarily to before, after we've extracted the matches we remove those lines from the file and try to find new ones. The approach here is that it i probably best to find all different names and see the possible IP addresses, then see if there are sole IP addresses without names. Sampling make it appear as if there are only IP addresses left.

```{r}
any_user <- paste(users, sep="", collapse="|")
all_identified_users <- matches(valids, any_user)
users_left <- setdiff(valids, all_identified_users)
sample(users_left, 5)
```

We shall begin by extracting The IP addresses of the known users and then the IP addresses from all non declared connections. Here are some possible differnet user ip combinations. At appears as if several users share the same ip adress. But there are no appearances of "cyrus", "news", or "test".

```{r}
ip_rx <- "[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}"
rx <- paste("(", any_user, ")", " from ", ip_rx, sep ="")
user_with_ip <- matches(all_identified_users, rx)
unique(str_extract(user_with_ip, rx))
```

It appears as if they are not related to an ip-adress. Thus it is whats seen above that are the different user ip adress combinations.

```{r}
rx <- "cyrus|news|test"
user_with_ip <- matches(all_identified_users, rx)
unique(user_with_ip)
```

For all of the ip, we have the following:

```{r}
ips <- matches(valids, ip_rx)
ips_isolated <- str_extract(ips, ip_rx)
valid_ips <- unique(ips_isolated)
valid_ips
```

## Find the invalid user login/ids

### What were the associated IPs

The invalid logins are made by 1826 unqiue different ip adresses.

```{r}
ips <- matches(invalids, ip_rx)
ips_isolated <- str_extract(ips, ip_rx)
invalid_ip <- unique(ips_isolated)
length(invalid_ip)
head(invalid_ip)
```


### Were there multiple invalid user logins from the same IP addresses

Below we see the top attempts. One has even tried thousands of times.

```{r}
top_attempts <- data.frame(table(ips_isolated)) %>% arrange(desc(Freq))
head(top_attempts, 10)
```


### Were there valid logins from those IP addresses

There appears to be overlaps. Five of them have invalid attempts.

```{r}
any(valid_ips %in% invalid_ip)
intersect(invalid_ip, valid_ips)
```


### Are there multiple IPs using the same invalid login?

First we find all the unique attempts with unique ips. Then, we remove the ips, and if there are more ocurrances of one attempt, it implies different ips has attempted the same login. Appear to be correctly extracted. Below we can observer that indeaed several different IPs use the same login usernames, most commonly "admin".

```{r}
invalid_rx <- paste("Invalid user ([^ ]+) from ", ip_rx, sep="")
unique_invalids <- unique(matches(invalids, invalid_rx))
unique_attempts <- gsub(ip_rx, "", unique_invalids)
sample(unique_attempts, 5)
res_df <- data.frame(table(unique_attempts)) %>% arrange(desc(Freq)) 
head(res_df, 10)
```

#### Are these related IPs, e.g., from the same network/domain?

Allthoug an IP is from the same network most of the time if only the last digits differ, it does not always hold (Nielsen, 2017). However, due to the scope of the assignment we will make such assumption. We want to take all the unique ocurrances, extract the ip, then compare similariteis to identify wheter they are from the same network or different. Luckily we have extracted the ip from before. We shall remove the last two "octet", count the ocurrances of the same network.

Below we see the most common occurances. From googling, it appears as the three most common ones are from Amazon, 1&1 Internet AG, and China Telecom (Find IP Address, 2023).

```{r}
rx <- "[0-9]{1,3}\\.[0-9]{1,3}"
res <- str_extract(invalid_ip, rx)
counts <- data.frame(table(res)) %>% arrange(desc(Freq))
head(counts, 10)
```

## What IP address had too many authentication failures.

Below we have found all unique rows with an overload message, matched which ones that contains an IP address, then extracted all the unique ocurrances. 

```{r}
unique(str_extract(matches(unique(overload), ip_rx), ip_rx))
```

# Sudo commands

```{r}
sudos <- 
    df %>%
    filter(app == "sudo") %>% 
    select(message) %>% 
    unique()
```

## What are the executables/programs run via sudo

```{r}
sudo_command <- matches(sudos$message, "COMMAND")
rx <- "(?<=COMMAND)([^ ]+)"
unique(str_extract(sudo_command, rx))
```

### By what user

```{r}
sudo_command <- matches(sudos$message, "USER=")
rx <- "(?<=USER=)([^ ]+)"
unique(str_extract(sudo_command, rx))
```


### What machine are they on?

Linux

# Sources

Find IP Address. (2023). IP - Lookup and Locator, Available Online: https://www.findip-address.com/ [Accessed 1 May 2023]

Nielsen, C. L. (2017). Answer to ‘Would IP Addresses with the Same First 3 Octets Necessarily Be from the Same Origin?’, Server Fault, Available Online: https://serverfault.com/a/828348 [Accessed 1 May 2023]

Pon, H. (2011). Answer to ‘“Incomplete Final Line” Warning When Trying to Read a .Csv File into R’, Stack Overflow, Available Online: https://stackoverflow.com/a/5996412 [Accessed 28 April 2023]

Sesek, R. (2016). BezelServices on OS X, Available Online: https://robert.sesek.com/2016/3/bezelservices_on_os_x.html [Accessed 29 April 2023]

sumo logic. (n.d.). What Is Syslog?, Available Online: https://www.sumologic.com/syslog/ [Accessed 29 April 2023]

